"""
Shared test configuration and fixtures for comprehensive testing.

This module provides common fixtures, utilities, and configuration
for all test modules to ensure consistency and maintainability.
"""

import os
import pytest
import logging
import tempfile
import shutil
from typing import Generator, Dict, Any
from unittest.mock import patch, MagicMock

# Configure test environment
os.environ.setdefault("MOCK_MODE", "true")
os.environ.setdefault("TESTING", "true")

try:
    from api import create_app  # type: ignore
    from database import db, DatabaseManager  # type: ignore
    from config import get_config  # type: ignore
except ImportError as e:
    pytest.fail(f"Failed to import required modules: {e}")

# Configure logging for all tests
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@pytest.fixture(scope="session")
def test_config():
    """Provide test configuration settings."""
    return {
        'timeout_seconds': 30,
        'polling_interval': 0.5,
        'max_retries': 3,
        'test_video_duration': 15,
        'cleanup_enabled': True
    }


@pytest.fixture(scope="session")
def app():
    """
    Create a Flask application configured for testing.
    
    This fixture is session-scoped to avoid repeated app creation.
    """
    try:
        app = create_app()
        app.config.update({
            'TESTING': True,
            'WTF_CSRF_ENABLED': False,
            'SQLALCHEMY_DATABASE_URI': 'sqlite:///:memory:',
        })
        
        with app.app_context():
            db.create_all()
            yield app
            db.drop_all()
            
    except Exception as e:
        pytest.fail(f"Failed to create test application: {e}")


@pytest.fixture
def client(app):
    """Create a test client for the Flask application."""
    try:
        with app.test_client() as client:
            yield client
    except Exception as e:
        pytest.fail(f"Failed to create test client: {e}")


@pytest.fixture
def database_manager(app):
    """Provide a database manager instance for testing."""
    try:
        with app.app_context():
            yield DatabaseManager(db)
    except Exception as e:
        pytest.fail(f"Failed to create database manager: {e}")


@pytest.fixture
def temp_directory():
    """Provide a temporary directory for test files."""
    temp_dir = tempfile.mkdtemp(prefix='test_video_gen_')
    try:
        yield temp_dir
    finally:
        try:
            shutil.rmtree(temp_dir)
        except Exception as e:
            logger.warning(f"Failed to cleanup temp directory {temp_dir}: {e}")


@pytest.fixture
def sample_video_data():
    """Provide standard test video data."""
    return {
        'title': 'Test Video',
        'description': 'A comprehensive test video for validation',
        'user_input': 'Create a test video demonstrating functionality',
        'style': 'educational',
        'duration': 30
    }


@pytest.fixture
def mock_external_services():
    """Mock external service dependencies for isolated testing."""
    with patch('agents.scene_generator.openai') as mock_openai, \
         patch('agents.audio_agent.openai') as mock_audio_openai, \
         patch('agents.video_agent.moviepy') as mock_moviepy:
        
        # Configure mock responses
        mock_openai.chat.completions.create.return_value = MagicMock(
            choices=[MagicMock(message=MagicMock(content='{"scenes": []}'))]
        )
        
        mock_audio_openai.audio.speech.create.return_value = MagicMock()
        mock_moviepy.VideoFileClip.return_value = MagicMock()
        
        yield {
            'openai': mock_openai,
            'audio_openai': mock_audio_openai,
            'moviepy': mock_moviepy
        }


class TestDataFactory:
    """Factory for creating test data with various scenarios."""
    
    @staticmethod
    def create_video_request(**overrides) -> Dict[str, Any]:
        """Create a video request with optional field overrides."""
        default_request = {
            'title': 'Factory Test Video',
            'description': 'Generated by TestDataFactory',
            'user_input': 'Create a factory-generated test video',
            'style': 'professional',
            'duration': 20
        }
        default_request.update(overrides)
        return default_request
    
    @staticmethod
    def create_invalid_requests() -> list:
        """Create a list of invalid request scenarios for testing."""
        return [
            {},  # Empty request
            {'title': ''},  # Empty title
            {'title': 'Valid', 'description': ''},  # Empty description
            {'title': 'Valid', 'description': 'Valid'},  # Missing user_input
            {'title': 'A' * 1000, 'description': 'Valid', 'user_input': 'Valid'},  # Title too long
            {'title': 'Valid', 'description': 'Valid', 'user_input': 'Valid', 'duration': -1},  # Invalid duration
            {'title': 'Valid', 'description': 'Valid', 'user_input': 'Valid', 'duration': 'invalid'},  # Wrong type
        ]
    
    @staticmethod
    def create_scene_data(count: int = 3) -> list:
        """Create test scene data."""
        scenes = []
        for i in range(count):
            scenes.append({
                'caption_text': f'Test scene {i+1} caption',
                'description': f'Description for test scene {i+1}',
                'duration': 5 + i
            })
        return scenes


@pytest.fixture
def test_data_factory():
    """Provide the test data factory."""
    return TestDataFactory


def pytest_configure(config):
    """Configure pytest with custom settings."""
    # Add custom markers
    config.addinivalue_line("markers", "slow: marks tests as slow (deselect with '-m \"not slow\"')")
    config.addinivalue_line("markers", "integration: marks tests as integration tests")
    config.addinivalue_line("markers", "e2e: marks tests as end-to-end tests")
    config.addinivalue_line("markers", "unit: marks tests as unit tests")


def pytest_runtest_setup(item):
    """Setup function run before each test."""
    # Ensure clean environment for each test
    if hasattr(item.cls, 'setup_method'):
        logger.info(f"Setting up test: {item.name}")


def pytest_runtest_teardown(item, nextitem):
    """Teardown function run after each test."""
    # Clean up any test artifacts
    if hasattr(item.cls, 'teardown_method'):
        logger.info(f"Tearing down test: {item.name}")


class TestErrorHandler:
    """Utility class for consistent error handling in tests."""
    
    @staticmethod
    def handle_expected_error(error_type, test_function, *args, **kwargs):
        """
        Handle expected errors in a consistent manner.
        
        Args:
            error_type: The expected exception type
            test_function: Function that should raise the error
            *args, **kwargs: Arguments to pass to the test function
        """
        try:
            test_function(*args, **kwargs)
            pytest.fail(f"Expected {error_type.__name__} but no exception was raised")
        except error_type as e:
            logger.info(f"Expected error caught: {e}")
            return e
        except Exception as e:
            pytest.fail(f"Expected {error_type.__name__} but got {type(e).__name__}: {e}")
    
    @staticmethod
    def assert_error_response(response, expected_status: int, should_have_error_field: bool = True):
        """
        Assert that a response indicates an error with proper structure.
        
        Args:
            response: Flask test response object
            expected_status: Expected HTTP status code
            should_have_error_field: Whether response should have error field
        """
        assert response.status_code == expected_status, \
            f"Expected status {expected_status}, got {response.status_code}"
        
        json_data = response.get_json()
        if json_data and should_have_error_field:
            assert json_data.get('success') is False or 'error' in json_data, \
                f"Error response should indicate failure: {json_data}"


@pytest.fixture
def error_handler():
    """Provide the error handler utility."""
    return TestErrorHandler


# Custom assertions for better test readability
def assert_valid_video_response(response_data: dict, expected_fields: list = None):
    """Assert that a response contains valid video data."""
    if expected_fields is None:
        expected_fields = ['id', 'title', 'description', 'status']
    
    assert isinstance(response_data, dict), "Video data should be a dictionary"
    assert 'video' in response_data, "Response should contain 'video' field"
    
    video_data = response_data['video']
    for field in expected_fields:
        assert field in video_data, f"Video should contain '{field}' field"
        assert video_data[field] is not None, f"Video '{field}' should not be None"


def assert_valid_progress_data(progress_data: list, min_entries: int = 1):
    """Assert that progress data is valid."""
    assert isinstance(progress_data, list), "Progress data should be a list"
    assert len(progress_data) >= min_entries, f"Should have at least {min_entries} progress entries"
    
    for i, entry in enumerate(progress_data):
        assert isinstance(entry, dict), f"Progress entry {i} should be a dictionary"
        # Check for common progress fields
        expected_fields = ['stage', 'step', 'status', 'message', 'timestamp', 'created_at']
        has_required_field = any(field in entry for field in expected_fields)
        assert has_required_field, f"Progress entry {i} missing required fields: {entry}"


# Export custom assertions for use in tests
__all__ = [
    'assert_valid_video_response',
    'assert_valid_progress_data',
    'TestDataFactory',
    'TestErrorHandler'
]
